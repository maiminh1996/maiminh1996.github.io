<!doctype html>
<html lang="en">
<meta http-equiv="content-type" content="text/html;charset=UTF-8" />
<head>
    <meta charset="UTF-8">
    <!--[if IE]><meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'><![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Nguyen Anh Minh MAI - Applied Scientist in Computer Vision</title>
    <!-- Default style sheets -->
    <link rel="shortcut icon" type="image/x-icon" href="images/logo_minh.png" />
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,700|Raleway:100,300" rel="stylesheet">
    <link rel="stylesheet" href="dist/dist.css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
      <![endif]-->
    <!--https://stackoverflow.com/questions/16751345/automatically-close-all-the-other-details-tags-after-opening-a-specific-detai/56194608
    <script language="javascript">alert("Chào mừng bạn đến với Hocban.vn");</script>-->
    <script language="javascript">
        const All_Details = document.querySelectorAll('details');

        All_Details.forEach(deet=>{
          deet.addEventListener('toggle', toggleOpenOneOnly)
        })

        function toggleOpenOneOnly(e) {
          if (this.open) {
            All_Details.forEach(deet=>{
              if (deet!=this && deet.open) deet.open = false
            });
          }
        }
    </script>
</head>


<body class="homepage ver-one" data-spy="scroll" data-target=".main-navigation">
    <!-- /#site-header -->
    <section id="about" class="default-section" data-scroll-reveal="enter from the bottom over .5s">
        <!-- /.section-heading -->
        <div class="about-container">
            <div class="container">
                <div class="col-md-15" data-scroll-reveal="enter from the right after .5s">
                    <div class="profile-details section-box">

                    <ins>Latest updates</ins>: 
                    <script type="text/javascript">
                        document.write(document.lastModified);
                    </script>
                    <h5 class="" style="text-align:center"><b>Nguyen Anh Minh MAI</b></h5>
                    <hr>
                    <div class="clearfix">
                        <div class="item_description">                                                    

                    <div class="aligncenter" style="background-image:url('');">
                        <!--<img width="250cm" src="images/icprs_21.png" alt="generated by A Neural Algorithm of Artistic Style paper">-->
                        <center><a href="https://www.linkedin.com/in/nguyen-anh-minh-mai/"><img width="210cm" style="border:1px solid black;" src="images/minh.png" alt="generated by A Neural Algorithm of Artistic Style paper"></a></center>
                    </div></div></div>
                    <br>
                    <!--<center><a href="https://www.linkedin.com/in/nguyen-anh-minh-mai/"><img style="border:1px solid black;" src="images/minh.png" alt="generated by A Neural Algorithm of Artistic Style paper" width="20%"></a></center><br><br><br>-->
                    
                    <h4>Summary</h4>
                    <hr>
                    <div class="about-content">
                        <p style="text-align:justify">I'm Nguyen Anh Minh MAI. I studied at the Centre-Val de Loire National Institute of Applied Sciences (<a href="https://www.groupe-insa.fr/en" target="_blank">INSA</a>) in Bourges, France, where I obtained my Diplôme d’Ingénieur (Master’s Degree).<br>
                        I am now a Ph.D. student in Computer Science at <a href="https://www.univ-tlse3.fr/english-version"  target="_blank">Paul Sabatier University</a> in Toulouse, France. I'm a member of <a href="https://www.irit.fr/en/departement/dep-signals-and-images/minds-team/"  target="_blank">MINDS</a> team of Toulouse Computer Science Research Institute (<a href="https://www.irit.fr/"  target="_blank">IRIT - UMR CNRS</a>) and a member of <a href="https://www.cerema.fr/fr/innovation-recherche/recherche/equipes/sti-systemes-transports-intelligents-plus-securite" target="_blank">STI</a> team of Toulouse Cerema Research Center (<a href="https://www.cerema.fr/fr/europe-international" target="_blank">Cerema</a>) in collaboration with an autonomous vehicle company <a href="https://easymile.com/" target="_blank">Easymile</a> in Toulouse, France.<br>
                        I am working on a project about environment perception (3D object detection and semantic segmentation on point cloud from camera & LiDAR sensors) for self-driving cars.</p>
                        <p style="text-align:justify">My current research is focusing on scene understanding (shape, depth, motion, object detection, and recognition) from images, videos, and 3D sensors such as LiDAR, Realsense.</p>
                        <p><ins>Fields of Interest</ins>: Artificial Intelligence, Backend Development, Quantitative research, Production ML systems</p>
                        <img src="https://img.shields.io/badge/-Python-333?style=flat-square&logo=Python&logoColor=white"><img src="https://img.shields.io/badge/-C/C++-c14438?style=flat-square&logo=C&logoColor=fff"><br>
                        <img src="https://img.shields.io/badge/-PyTorch-e34f26?style=flat-square&logo=PyTorch&logoColor=fff"><img src="https://img.shields.io/badge/-TensorFlow-yellow?style=flat-square&logo=TensorFlow&logoColor=orange"><img src="https://img.shields.io/badge/-Flask-black?style=flat-square&logo=flask&logoColor=white"><br>
                        <img src="https://img.shields.io/badge/-Git-F05032?style=flat-square&logo=git&logoColor=white"><img src="https://img.shields.io/badge/-Linux-FCC624?style=flat-square&logo=linux&logoColor=black"><img src="https://img.shields.io/badge/-Docker-2496ED?style=flat-square&logo=docker&logoColor=white"><br><br>
                        <p><ins>My CV</ins>: <a href="doc/Minh_CV.pdf" target="_blank">Minh_CV.pdf</a></p><br>

                        <h4>Contact</h4>
                        <hr>

                        <p style="text-align:justify"><ins>Email</ins>: <a href="mailto:mainguyenanhminh1996@gmail.com">mainguyenanhminh1996@gmail.com</a></p>
                        <p style="text-align:justify"><ins>Socials</ins>: <a href="https://www.linkedin.com/in/nguyen-anh-minh-mai/"  target="_blank">LinkedIn</a> / <a href="https://github.com/maiminh1996"  target="_blank">GitHub</a> / <a href="https://www.researchgate.net/profile/Nguyen-Anh-Minh-Mai"  target="_blank">ResearchGate</a> / <a href="https://scholar.google.fr/citations?hl=en&user=lxTC9IIAAAAJ" target="_blank">Google Scholar</a></p></li><br>

                        <h4 style="color:red;">News</h4>
                        <hr>

                        <li><p style="text-align:justify"><kbd1>13/ Sep/ 2021</kbd1> My <a href="https://orasis2021.sciencesconf.org/"  target="_blank">ORASIS 2021</a> paper is now available on <a href="https://hal.archives-ouvertes.fr/hal-03339637/document"  target="_blank">HAL</a>.</p></li>
                        <li><p style="text-align:justify"><kbd1>23/ Mar/ 2021</kbd1> I will be teaching assistant at <a href="https://rlvs.aniti.fr/"  target="_blank">Reinforcement Learning Virtual School (RLVS)</a> hosted by the <a href="https://aniti.univ-toulouse.fr/"  target="_blank">Artificial and Natural Intelligence Toulouse Institute (ANITI)</a> which will last a total of six days: March 25-26, April 1-2, and April 8-9, 2021.</p></li>

                        <li><p style="text-align:justify"><kbd1>19/ Mar/ 2021</kbd1> I got the Best Paper Award at the <a href="http://www.icprs.org/"  target="_blank">ICPRS 2021</a>.</p></li>

                        <li><p style="text-align:justify"><kbd1>17/ Mar/ 2021</kbd1> I gave an <a href="doc/ICPRS_talk.pdf"  target="_blank">oral presentation</a> at the <a href="http://www.icprs.org/"  target="_blank">ICPRS 2021</a>.</p></li>

                        <li><p style="text-align:justify"><kbd1>05/ Mar/ 2021</kbd1> A preprint of my ICPRS 2021 paper "Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimation and 3D Object Detection" is available on <a href="https://arxiv.org/abs/2103.03977"  target="_blank">arXiv</a>.</p></li>

                        <li><p style="text-align:justify"><kbd1>02/ Jan/ 2021</kbd1> My paper about LiDAR and stereo fusion for 3D object detection that was submitted to the <a href="http://www.icprs.org/"  target="_blank">11th International Conference on Pattern Recognition Systems (ICPRS 2021)</a> got accepted for publication and oral presentation.</p></li></br>

                        <h4>Publications</h4>
                        <hr>



                        <div class="clearfix">
                            <div class="item_description">
                                <div class="image" style="background-image:url('');">
                                    <img width="250cm" src="images/fog_21.png" alt="">
                                </div>
                            <div class="desc" >
				<p style="text-align:justify">
                                <b>Détection d’obstacles par vision et LiDAR par temps de brouillard pour les véhicules autonomes</b><br>
                                ORASIS 2021<br>
                                <ins>Nguyen Anh Minh Mai</ins>, Pierre Duthon, Louahdi Khoudour, Alain Crouzil, Sergio A. Velastin<br></p>
                                <kbd1><a href="https://hal.archives-ouvertes.fr/hal-03339637/document"  target="_blank">HAL</kbd1></a> <kbd1><a href="https://github.com/maiminh1996/SLS-Fusion"  target="_blank">Project page</a></kbd1>
                                <details style="display:inline;">
                                    <summary><kbd1>Abstract</kbd1></summary>
                                    <p style="text-align:justify">
                                        This work concerns the generation of a synthetic fog dataset based on available datasets in good weather conditions. A synthetic dataset is necessary because it is not always possible to collect real data under degraded conditions. In addition, post-processing such as labeling or filtering datais not easy and time-consuming. A 3D object detection algorithm for autonomous vehicles is then implemented andevaluated on the dataset produced in order to analyze theimpact of the weather on its performance. In the light ofthe results obtained, perspectives are proposed to improveperformance of the proposed method earlier.
                                    </p>
                                </details>
                                <details style="display:inline;">
                                    <summary><kbd1>BibTeX</kbd1></summary>
                                    <pre><code>@inproceedings{mai:hal-03339637,
  TITLE = {{D{\'e}tection d'obstacles par vision et LiDAR par temps de brouillard pour lesv{\'e}hicules autonomes}},
  AUTHOR = {MAI, Nguyen Anh Minh and Duthon, Pierre and Crouzil, Alain and Khoudour, Louahdi and A. Velastin, Sergio},
  URL = {https://hal.archives-ouvertes.fr/hal-03339637},
  BOOKTITLE = {{18{\`e}mes  journ{\'e}es francophones des jeunes chercheurs en vision par ordinateur (ORASIS 2021)}},
  ADDRESS = {Saint Ferr{\'e}ol, France},
  ORGANIZATION = {{Centre National de la Recherche Scientifique [CNRS] and Equipe REVA, IRIT : Institut de Recherche en Informatique de Toulouse.}},
  YEAR = {2021},
  MONTH = Sep,
  KEYWORDS = {foggy driving scenes ; autonomous vehicles ; 3D object detection ; synthe-tic datasets ; adverse weather conditions ; v{\'e}hicules autonomes ; jeux de donn{\'e}es synth{\'e}tiques ; sc{\`e}nes de conduite dans le brouillard ; Conditions m{\'e}t{\'e}orologiques d{\'e}favorables ; d{\'e}tection d'objets 3D},
  PDF = {https://hal.archives-ouvertes.fr/hal-03339637/file/Minh_ORASIS_2_pages.pdf},
  HAL_ID = {hal-03339637},
  HAL_VERSION = {v1},
}
</code></pre>
                                </details>
                                <!--<video width="320" height="240" align="MIDDLE" controls>
                                <source src="al.mp4" type="video/mp4">
                                Your browser does not support the video tag
                                </video>-->
                            </div>
                            </div>
                        </div>


<br>



                        <div class="clearfix">
                            <div class="item_description">
                                <div class="image" style="background-image:url('');">
                                    <img width="250cm" src="images/icprs_21.png" alt="generated by A Neural Algorithm of Artistic Style paper">
                                </div>
                            <div class="desc" >
				<p style="text-align:justify">
                                <b>Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimation and 3D Object Detection</b><br>
                                ICPRS 2021<br>
                                <ins>Nguyen Anh Minh Mai</ins>, Pierre Duthon, Louahdi Khoudour, Alain Crouzil, Sergio A. Velastin<br></p>
                                <kbd1><a href="https://arxiv.org/abs/2103.03977"  target="_blank">arXiv</kbd1></a> <kbd1><a href="https://github.com/maiminh1996/SLS-Fusion"  target="_blank">Project page</a></kbd1>
                                <details style="display:inline;">
                                    <summary><kbd1>Abstract</kbd1></summary>
                                    <p style="text-align:justify">
                                        The ability to accurately detect and localize objects is recognized as being the most important for the perception of self-driving cars. From 2D to 3D object detection, the most difficult is to determine the distance from the ego-vehicle to objects. Expensive technology like LiDAR can provide a precise and accurate depth information, so most studies have tended to focus on this sensor showing a performance gap between LiDAR-based methods and camera-based methods. Although many authors have investigated how to fuse LiDAR with RGB cameras, as far as we know there are no studies to fuse LiDAR and stereo in a deep neural network for the 3D object detection task. This paper presents SLS-Fusion, a new approach to fuse data from 4-beam LiDAR and a stereo camera via a neural network for depth estimation to achieve better dense depth maps and thereby improves 3D object detection performance. Since 4-beam LiDAR is cheaper than the well-known 64-beam LiDAR, this approach is also classified as a low-cost sensors-based method. Through evaluation on the KITTI benchmark, it is shown that the proposed method significantly improves depth estimation performance compared to a baseline method. Also, when applying it to 3D object detection, a new state of the art on low-cost sensor based method is achieved.
                                    </p>
                                </details>
                                <details style="display:inline;">
                                    <summary><kbd1>BibTeX</kbd1></summary>
                                    <pre><code>@article{mai2021sparse,
  title={Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D Object Detection},
  author={Mai, Nguyen Anh Minh and Duthon, Pierre and Khoudour, Louahdi and Crouzil, Alain and Velastin, Sergio A},
  journal={International Conference on Pattern Recognition Systems (ICPRS)},
  year={2021}}</code></pre>

                                </details>
                                <!--<video width="320" height="240" align="MIDDLE" controls>
                                <source src="al.mp4" type="video/mp4">
                                Your browser does not support the video tag
                                </video>-->
                            </div>
                            </div>
                        </div>








                    </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <p style="text-align:center; color:white;">Copyright © 2021 Nguyen Anh Minh MAI</p>
    <script defer="defer" async="async" src="dist/dist.js"></script>
</body>
</html>
