<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Loss function | MAI Minh's Page </title> <meta name="author" content="N. A. Minh MAI"> <meta name="description" content="Computer Science Blog | Nguyen Anh Minh MAI, AI Research Engineer "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/logo_minh.png?a7cf9880c438f18bb5f6425fa92f16c2"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://maiminh1996.github.io//blog/2022/loss-function/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> MAI Minh's Page </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/repositories/">repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Loss function</h1> <p class="post-meta"> Created in November 24, 2022 by MAI Minh </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> pytorch</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Some basic loss defined in pytorch. Sometimes loss definition in <code class="language-plaintext highlighter-rouge">torch.nn</code> (<code class="language-plaintext highlighter-rouge">nn</code>) and <code class="language-plaintext highlighter-rouge">torch.nn.functional</code> (<code class="language-plaintext highlighter-rouge">F</code>) can confuse us.</p> <p>Notes:</p> <ul> <li>regression: predict a real-value quantity</li> <li>classification: predict a probability (need a non-linear activation function)</li> </ul> <h2 id="negative-log-likelihood-loss">Negative Log Likelihood Loss</h2> <blockquote> <p>[classification] It is useful to train a classification problem with C classes.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">nn.NLLLoss(input, target)</code> == <code class="language-plaintext highlighter-rouge">F.nll_loss()</code></p> <ul> <li>input: (N, C), or $(N, C, d_{1}, d_{2}, …, d_K)$ with $K \geq 1$ in the case of K-dimensional loss.</li> <li>target: (N), or $(N, d_{1}, d_{2}, …, d_{K})$ with $K \geq 1$ in the case of K-dimensional loss.</li> <li>output: scalar. If reduction is none, then the same size as the target: (N), or $(N, d_{1}, d_{2}, …, d_{K})$ with $K \geq 1$ in the case of K-dimensional loss.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">NLLLoss</span><span class="p">()</span>
<span class="c1"># input is of size N x C = 3 x 5
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># each element in target has to have 0 &lt;= value &lt; C
</span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="nf">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="nf">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span> <span class="c1"># scalar
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 2D loss example (used, for example, with image inputs)
</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">NLLLoss</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># (N, C, H, W)
</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># dim --&gt;
# each element in target has to have 0 &lt;= value &lt; C
</span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="nf">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># (N, 8, 8)
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">m</span><span class="p">(</span><span class="nf">conv</span><span class="p">(</span><span class="n">data</span><span class="p">))</span> <span class="c1"># (N, C, 8, 8)
</span><span class="n">l</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="c1"># scalar
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># loss example (used, for example, with pcl inputs)
</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">6</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">NLLLoss</span><span class="p">()</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="c1">#, feature_transform
</span><span class="n">pcl</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># (N, L, 3), 3: x, y, z
</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">m</span><span class="p">(</span><span class="nf">fc2</span><span class="p">(</span><span class="nf">fc1</span><span class="p">(</span><span class="n">pcl</span><span class="p">)))</span> <span class="c1"># (N, L, C), m dim: -1 or 2
</span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="nf">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># (N, L)
</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (N*L)
</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># (N*L, C)
</span><span class="n">l</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="c1"># scalar
</span></code></pre></div></div> <h2 id="cross-entropy-loss">Cross Entropy Loss</h2> <blockquote> <p>[classification] Cross Entropy Loss, also called logarithmic loss, log loss or logistic loss</p> </blockquote> \[L_{CE} = - \sum_{i=1}^{n} t_{i}log(p_{i}),\] <p>where, $t_{i}$ is the truth label and $p_{i}$ is the softmax probability for $i^{th}$ class.</p> <p><code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss(input, target)</code><br>== <code class="language-plaintext highlighter-rouge">F.cross_entropy()</code> == <code class="language-plaintext highlighter-rouge">F.nll_loss(F.log_softmax(input, 1), target)</code>. <span style="color:red">In Pytorch, these criterion combines <code class="language-plaintext highlighter-rouge">log_softmax</code> and <code class="language-plaintext highlighter-rouge">nll_loss</code> in a single function</span>.</p> <ul> <li>input: (N, C), (N, C, H, W)</li> <li>target: (N) where each value is $0 \leq \text{targets}[i] \leq C-1$, or $(N, d_1, d_2, …, d_K)$ where $K \geq 1$ for K-dimensional loss.</li> <li>output: scalar. If reduction is none, then the same size as the target (N), or $(N, d_1, d_2, …, d_K)$ with $K \geq 1$ in the case of K-dimensional loss.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span>

<span class="c1"># Example of target with class indices
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">).</span><span class="nf">random_</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="c1"># scalar
</span>
<span class="c1"># Example of target with class probabilities
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">).</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div></div> <h2 id="binary-cross-entropy">Binary Cross Entropy</h2> <blockquote> <p>[classification] Cross Entropy Loss, also referred as Logarithmic loss</p> </blockquote> <p>The problem is framed as predicting the likelihood of an example belonging to class one, e.g. the class that you assign the integer value 1, whereas the other class is assigned the value 0.</p> <p><code class="language-plaintext highlighter-rouge">nn.BCELoss(input, target)</code> == <code class="language-plaintext highlighter-rouge">F.binary_cross_entropy()</code></p> <ul> <li>input: Tensor of arbitrary shape</li> <li>target: Tensor of the same shape as input</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div></div> <p>Function that measures Binary Cross Entropy between target and output logits. This loss combines a <code class="language-plaintext highlighter-rouge">Sigmoid</code> layer and the <code class="language-plaintext highlighter-rouge">BCELoss</code> in one single</p> <p><code class="language-plaintext highlighter-rouge">nn.BCEWithLogitsLoss(input, target)</code> == <code class="language-plaintext highlighter-rouge">F.binary_cross_entropy_with_logits()</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BCEWithLogitsLoss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div></div> <h2 id="smooth-l1-loss">Smooth L1 Loss</h2> <blockquote> <p>[regression]</p> </blockquote> <p>Function that uses a <a href="#mse-loss">squared term</a> if the absolute element-wise error falls below $\beta$ (1 by default) and an <a href="#mae-loss">L1 term</a> otherwise. It is less sensitive to outliers than the <a href="#mse-loss">MSELoss</a> and in some cases prevents exploding gradients (e.g. see Fast R-CNN paper by Ross Girshick).</p> \[L_{Smooth\ L1} = \frac{1}{n} \sum_{i} z_{i}\] <p>where $z_{i}$ is given by:</p> \[z_{i} = \begin{cases} 0.5 (x_i - y_i)^2 / \beta, &amp; \text{if } |x_i - y_i| &lt; \beta \\ |x_i - y_i| - 0.5 * \beta, &amp; \text{otherwise } \end{cases}\] <p>$x$ and $y$ arbitrary shapes with a total of $n$ elements each the sum operation still operates over all the elements, and divides by $n$.</p> <p><code class="language-plaintext highlighter-rouge">nn.SmoothL1Loss(input, target)</code> == <code class="language-plaintext highlighter-rouge">F.smooth_l1_loss()</code></p> <ul> <li>input: (*)</li> <li>target: (*), same shape as the input.</li> <li>output: scalar. If reduction is ‘none’, then (*), same shape as the input.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">SmoothL1Loss</span><span class="p">()</span> <span class="c1"># default reduction='mean'
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="c1"># scalar
</span></code></pre></div></div> <h2 id="mse-loss">MSE Loss</h2> <blockquote> <p>[regression] Mean Squared Error (MSE) Loss, also called L2 norm</p> </blockquote> \[L_{MSE} = \frac{1}{n}\sum_{i=1}^{n}\left ( Y_{i} - \hat{Y}_{i} \right )^{2}\] <p><code class="language-plaintext highlighter-rouge">nn.MSELoss(input, target)</code> == <code class="language-plaintext highlighter-rouge">F.mse_loss()</code></p> <ul> <li>input: (N, *)</li> <li>target: (N, *)</li> <li>ouput: scalar</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span> <span class="c1"># default reduction='mean'
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="c1"># scalar
# torch.square(input-target).mean()
</span>
<span class="c1"># loss1 = nn.MSELoss(reduction='sum')
# output1 = loss1(input, target) # scalar
# torch.square(input-target).sum()
</span></code></pre></div></div> <h2 id="l2-loss">L2 Loss</h2> <p>L2 loss is another name for <a href="#mse-loss">Mean Squared Error (MSE) Loss</a>.</p> <h2 id="mae-loss">MAE Loss</h2> <blockquote> <p>[regression] Mean Absolute Error (MAE) Loss, also called L1 norm</p> </blockquote> \[L_{MAE} = \frac{1}{n}\sum_{i=1}^{n}\left | Y_{i} - \hat{Y}_{i} \right |\] <p><code class="language-plaintext highlighter-rouge">nn.L1Loss(input, target)</code> == <code class="language-plaintext highlighter-rouge">F.l1_loss()</code> == <code class="language-plaintext highlighter-rouge">nn.L1Loss()</code></p> <ul> <li>input: (N, *)</li> <li>target: (N, *)</li> <li>ouput: scalar</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">L1Loss</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="c1"># torch.abs(input-target).mean()
</span>
<span class="c1"># loss1 = nn.L1Loss(reduction='sum')
# output1 = loss1(input, target) # scalar
# torch.abs(input-target).sum()
</span></code></pre></div></div> <h2 id="l1-loss">L1 Loss</h2> <p>L1 loss is another name for <a href="#mae-loss">Mean Absolute Error (MAE) Loss</a>.</p> <h2 id="references">References</h2> <ol> <li><a href="https://pytorch.org/docs/stable/nn.functional.html#loss-functions" rel="external nofollow noopener" target="_blank">https://pytorch.org/docs/stable/nn.functional.html#loss-functions</a></li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/c-testing-with-gtest/">C++ Testing with GTest</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tmux/">Tmux: An Introduction to Terminal Multiplexing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/vim/">Basic Vim Usage</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/python-testing-with-pytest/">Python Testing with pytest</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/docker-inside-vs-code/">Docker inside VS Code</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="https-maiminh1996-github-io-5",disqus_identifier="/blog/2022/loss-function",disqus_title="Loss function";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 N. A. Minh MAI. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-repositories",title:"repositories",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-c-testing-with-gtest",title:"C++ Testing with GTest",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/c-testing-with-gtest/"}},{id:"post-tmux-an-introduction-to-terminal-multiplexing",title:"Tmux: An Introduction to Terminal Multiplexing",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/tmux/"}},{id:"post-basic-vim-usage",title:"Basic Vim Usage",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/vim/"}},{id:"post-python-testing-with-pytest",title:"Python Testing with pytest",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/python-testing-with-pytest/"}},{id:"post-docker-inside-vs-code",title:"Docker inside VS Code",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/docker-inside-vs-code/"}},{id:"post-expert-advice-for-successful-research",title:"Expert Advice for Successful Research",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/research/"}},{id:"post-good-advice-and-tips",title:"Good advice and tips",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/good-advice-and-tips/"}},{id:"post-docker",title:"Docker",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/docker/"}},{id:"post-loss-function",title:"Loss function",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/loss-function/"}},{id:"post-glossary-ml",title:"Glossary ML",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/glossary/"}},{id:"post-disable-gradient-computation",title:"Disable gradient computation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/disable-gradient-computation/"}},{id:"post-mastering-google-colab-tips-and-tricks",title:"Mastering Google Colab: Tips and Tricks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/google-colab/"}},{id:"post-linear-algebra",title:"linear algebra",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/algebra/"}},{id:"post-python-data-structure",title:"Python data structure",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/python-data-structure/"}},{id:"post-math",title:"Math",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/math/"}},{id:"post-git-commit-message",title:"git Commit Message",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/git-commit-message/"}},{id:"post-git-best-pratice",title:"git best pratice",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/git-best-pratice/"}},{id:"post-setting-up-vs-code",title:"Setting up VS Code",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/vscode/"}},{id:"post-git-synthesis",title:"Git Synthesis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/git/"}},{id:"post-installing-cuda",title:"Installing CUDA",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/installing-cuda/"}},{id:"post-installing-opencv",title:"Installing OpenCV",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/installing-opencv/"}},{id:"post-pihole-camera",title:"Pihole Camera",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/pihole-camera/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%61%69%6E%67%75%79%65%6E%61%6E%68%6D%69%6E%68%31%39%39%36@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=lxTC9IIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/maiminh1996","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/nguyen-anh-minh-mai","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>